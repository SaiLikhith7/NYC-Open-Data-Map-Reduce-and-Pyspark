# NYC-Open-Data-Map-Reduce-and-Pyspark
## subheading
![Custom Badge](https://img.shields.io/badge/Sai%20Likhith-NYC%20Open%20Data-Green)
![Issues](https://img.shields.io/github/issues/SaiLikhith7/NYC_Open_Data-Map-Reduce-and-Pyspark)


`sailikhith` da

This repository contains the Map and Reduce functionality of big data sets using PySpark.

 
## Key Difference Between MapReduce vs Apache Spark
- MapReduce is strictly disk-based while Apache Spark uses memory and can use a disk for processing.
- MapReduce and Apache Spark both have similar compatibility in terms of data types and data sources.
- The primary difference between MapReduce and Spark is that MapReduce uses persistent storage and Spark uses Resilient Distributed Datasets.
- Hadoop MapReduce is meant for data that does not fit in the memory whereas Apache Spark has a better performance for the data that fits in the memory, particularly on dedicated clusters.
- Hadoop MapReduce can be an economical option because of Hadoop as a service and Apache Spark is more cost effective because of high availability memory
- Apache Spark and Hadoop MapReduce both are failure tolerant but comparatively Hadoop MapReduce is more failure tolerant than Spark.
- Hadoop MapReduce requires core java programming skills while Programming in Apache Spark is easier as it has an interactive mode.
- Spark is able to execute batch-processing jobs between 10 to 100 times faster than the MapReduce Although both the tools are used for processing Big Data.
## When to use MapReduce:

- Linear Processing of large Dataset
- No intermediate Solution required
## When to use Apache Spark:

- Fast and interactive data processing
- Joining Datasets
- Graph processing
- Iterative jobs
- Real-time processing
- Machine Learning
